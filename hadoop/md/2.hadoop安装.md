1. 确定 Hadoop 版本
   ```shell
   $ pyspark
   >>> sc._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()
   '3.3.2'
   ```

2. 获取 Hadoop 3.3.2 的安装包，下载地址：https://archive.apache.org/dist/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
   > 注：推荐使用中科大镜像:<br>
   > https://mirrors.ustc.edu.cn/apache/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
   
3. 准备配置文件及启动脚本
   - 在工作目录下创建 config 文件夹（如：mkdir /opt/docker/spark/config）
   - 在 config 目录下编写需要覆盖的 Hadoop 配置文件（具体配置见 /spark/config）
      ```shell
      tree
      .
      ├── core-site.xml
      ├── hadoop-env.sh
      ├── hdfs-site.xml
      ├── mapred-site.xml
      ├── ssh_config
      ├── workers
      └── yarn-site.xml

      0 directories, 7 files
      ```
   - 准备 Hadoop 启动脚本（start-hadoop.sh）
      ```bash
      #!/bin/bash
      # 启动 ssh 服务（用于 ssh 免密通信）
      service ssh start
      # 依次启动 HDFS 和 YARN 集群
      $HADOOP_HOME/sbin/start-dfs.sh
      $HADOOP_HOME/sbin/start-yarn.sh
      ```

4. 基于 bitnami/spark 构建新镜像
   - 在工作目录下创建 Dockerfile
      ```dockerfile
      FROM docker.io/bitnami/spark:3.3
      LABEL description="Docker image with Spark (3.3.1) and Hadoop (3.3.2), based on bitnami/     spark:3.3 ."

      USER root

      # 设置 Hadoop 环境变量
      ENV HADOOP_HOME="/opt/hadoop"
      ENV HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"
      ENV HADOOP_LOG_DIR="/var/log/hadoop"
      ENV PATH="$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH"

      WORKDIR /opt

      # 更换镜像源
      RUN sed -i 's#deb.debian.org#mirrors.tuna.tsinghua.edu.cn#g' /etc/apt/sources.list

      # 安装 SSH 服务和 curl
      RUN apt-get update && \ 
      apt-get install -y openssh-server && \
      apt-get install -y curl

      # 配置免密登陆
      RUN ssh-keygen -t rsa -f /root/.ssh/id_rsa -P '' && \
          cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

      # 下载安装 Hadoop
      RUN curl -OL https://mirrors.ustc.edu.cn/apache/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
      RUN tar -zxvf hadoop-3.3.2.tar.gz && \
          mv hadoop-3.3.2 hadoop && \
          rm -rf hadoop-3.3.2.tar.gz && \
          mkdir /var/log/hadoop

      # 创建 HDFS NameNode 和 DataNode 工作目录
      RUN mkdir -p /root/hdfs/namenode && \
          mkdir -p /root/hdfs/datanode 

      COPY config/* /tmp/

      # 覆盖 $HADOOP_CONF_DIR 目录下的 Hadoop 配置文件
      RUN mv /tmp/ssh_config /root/.ssh/config && \
          mv /tmp/hadoop-env.sh $HADOOP_CONF_DIR/hadoop-env.sh && \
          mv /tmp/hdfs-site.xml $HADOOP_CONF_DIR/hdfs-site.xml && \ 
          mv /tmp/core-site.xml $HADOOP_CONF_DIR/core-site.xml && \
          mv /tmp/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml && \
          mv /tmp/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml && \
          mv /tmp/workers $HADOOP_CONF_DIR/workers

      # 拷贝 Hadoop 启动脚本并设置为可执行文件
      COPY start-hadoop.sh /opt/start-hadoop.sh

      RUN chmod +x /opt/start-hadoop.sh && \
          chmod +x $HADOOP_HOME/sbin/start-dfs.sh && \
          chmod +x $HADOOP_HOME/sbin/start-yarn.sh

      # 格式化 HDFS 文件系统
      RUN hdfs namenode -format

      ENTRYPOINT [ "/opt/bitnami/scripts/spark/entrypoint.sh" ]
      CMD [ "/opt/bitnami/scripts/spark/run.sh" ]
      ```
   - 构建 Spark-Hadoop:3 镜像
      ```shell
      docker image build -t spark-hadoop:3 .
      ```

5. 启动 spark-hadoop 集群
   - 构建完镜像后，修改 `docker-compose.yml` 文件
      ```yml
      
      ```
